{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward & Backward Passes from Foundations\n",
    "\n",
    "#### Last Time\n",
    "In the [previous notebook](http://nbviewer.jupyter.org/github/jamesdellinger/fastai_deep_learning_course_part2_v3/blob/master/01_matmul_my_reimplementation.ipynb?flush_cache=true), we compared and contrasted techniques for implementing matrix multiplication from scratch. We saw that computation time decreased as we moved from using pure Python for-loops to broadcasting, and then we sped things up even further by employing [einstein summation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n",
    "\n",
    "Finally, and admittedly somewhat disappointingly, we confirmed that using the built-in PyTorch matmul method offered the fastest performance, thanks to the fact that it hands the operation off to the Nividia [BLAS](https://docs.nvidia.com/cuda/index.html), whose low-level routines have been optimized to perform linear algebraic calculations as rapidly as possible on Nvidia GPUs. The somewhat disappointing aspect of this is that we, as developers, don't have any vision into exactly what's going on inside Nvidia's subroutines. We have to take it at its word that it's doing things the best possible way, and while its speed would seem to indicate that this is the case, it still would be nice to be able to write our code in the language that the compiler will be interpreting.\n",
    "\n",
    "#### Forward and Backward Passes\n",
    "In this notebook we follow the same pattern as the last and experiment with various approaches to implementing forward and back propagation for a fully connected neural network. We'll continue to use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset to observe how our code performs.\n",
    "\n",
    "#### Weight Initialization\n",
    "During the course of this notebook, I also embark on an exploration of the reasons why properly initializing neural network weight layers is so important. Much of my work is inspired by a [notebook](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02b_initializing.ipynb) that was covered during the [2nd lesson (aka \"Lesson 9\")](https://youtu.be/AcA8HAYh7IE) lecture of the 2019 fast.ai Deep Learning part II course.\n",
    "\n",
    "I show a series of simple experiments that illustrate the hows and whys of weight init, while touching on previous work done by [Glorot and Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), and by [He, et. al.](https://arxiv.org/pdf/1502.01852.pdf) Much of my work in this notebook served as a basis for a [medium post](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79?source=---------2------------------) I wrote on the topic of weight init.\n",
    "\n",
    "The [2nd lesson (\"Lesson 9\")](https://youtu.be/AcA8HAYh7IE) also covered [this notebook](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02a_why_sqrt5.ipynb) that explored the history behind the PyTorch library's use of $\\sqrt{5}$, instead of the Kaiming paper's recommended value of $\\sqrt{2}$ when generating a Kaiming weight initialization. I omit this side bar from my discussion in this notebook, but suffice to say, we learned in our lecture that the use of $\\sqrt{5}$ was due to a long-standing bug that members of the PyTorch team throught worked better than the paper's default of $\\sqrt{2}$. [Jeremy's notebook](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02a_why_sqrt5.ipynb) demonstrated that this was not quite the case, and the PyTorch team has recently revised the algorithm for calculating Kaiming, removing the $\\sqrt{5}$.\n",
    "\n",
    "#### Attribution\n",
    "Virtually all the code that appears in this notebook is the creation of [Sylvain Gugger](https://www.fast.ai/about/#sylvain) and [Jeremy Howard](https://www.fast.ai/about/#jeremy). The original versions of the notebooks that they made for the course lecture, from which I drew the material that appears below, can be found [here](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02_fully_connected.ipynb) and [here](https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb). I simply re-typed, line-by-line, the pieces of logic necessary to implement the functionality that their notebooks demonstrated. In some cases I changed the order of code cells and or variable names so as to fit an organization and style that seemed more intuitive to me. Any and all mistakes are my own.\n",
    "\n",
    "On the other hand, all long-form text explanations in this notebook are solely my own creation. Writing extensive descriptions of the concepts and code in plain and simple English forces me to make sure that I actually understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from exports.nb_01 import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train, y_train, x_valid, y_valid)) # convert train/val to torch tensors\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a neural network on our MNIST images, we first need to modify the floating point values inside the tensors that contain each image, such that across all the values of all the images in the training set, the values fall into a normal distribution.\n",
    "\n",
    "If we were to skip this step, and the gaps between different values were too skewed, it would take much longer, or possibly be impossible for our network to learn patterns from the MNIST digit handwriting images. More specifically, if two pixels are more similar than different, without proper normalization of their numerical values, it may be highly difficult for the network to understand that the pixels are in fact similar. The same would also be true for learning that two very different pixels are in fact different.\n",
    "\n",
    "Furthermore, as we will see below, different approaches for network layer weight initialization presume that inputs will fall into a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right below that straight out of the box, our imageset's numerical values don't follow a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, x_train.mean(), x_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0001), tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values inside the images in the training set now all follow a normal distribution, with a mean and standard deviation hovering around 0 and 1, respectively.\n",
    "\n",
    "We also need to normalize the images in the validation set. The key thing here is to remember that since this is a validation set, we can't allow any information from its images to leak into our model. Therefore, while we do normalize it, we do so by using the mean and standard deviation of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1287), tensor(0.3050))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.mean(), x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to use training mean and standard dev to normalize val set!\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0057), tensor(0.9924))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.mean(), x_valid.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_near_zero(a, tol=1e-3): assert a.abs()<tol, f'Near zero: {a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(x_train.mean())\n",
    "test_near_zero(1 - x_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Foundations Version of Forward/Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first write the logic for forward and backward passes using several simple methods. Later on in the notebook, we'll refactor these into better-organized classes.\n",
    "\n",
    "Our network's hidden layer will contain 50 cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "\n",
    "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and networks will take longer to converge, if they are even able to do so at all. \n",
    "\n",
    "Let's first think about initial weight values in the context of how neural network layers work. Matrix multiplication is the basic operation in a neural network. Here's a quick-and-dirty thought exercise to help us get a conceptual feel for why weight initialization matters in neural nets with several layers. i.e. neural nets where matrix multiplications are performed over and over in succession.\n",
    "\n",
    "Let's pretend we have a vector `x` that contains some network inputs, and that the matrix `a` represents a layer's weights.\n",
    "\n",
    "It is the standard practice when training neural networks to ensure that our inputs' values are scaled such that they fall inside such a normal distribution with a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it turns out that also initializing the values of layer input weights from the same standard normal distribution is never a good idea. To see why this is, let's run through a quick example where we do just that, and see what happens. \n",
    "\n",
    "Say we had a simple hundred-layer network with no activations, in order to complete a single forward pass we'd have to perform a single matrix multiplication between layer inputs and weights at each of the hundred layers, for a grand total of *100* consecutive matrix multiplications.\n",
    "\n",
    "We can simulate this by multiplying `x` by `a` 100 times in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100): \n",
    "    a = torch.randn(512,512)\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! Somewhere during 100 multiplications, the outputs got so big that even the computer wasn't able to recognize their standard deviation and mean as numbers. We can see exactly how long that took to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = torch.randn(512,512)\n",
    "    x = a @ x\n",
    "    if torch.isnan(x.std()): break\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation outputs exploded within 29 of our hypothetical network \"layers.\" We clearly initialized our weights (the `a` matrix) to be too large.\n",
    "\n",
    "Now let's see the other side of the coin: what happens when we initialize network weights to be too small -- we'll scale our initial weights values so that, although still being in a normal distribution with a mean about 0, they have a standard deviation of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.randn(512,512) * 0.01\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course of the above hypothetical forward pass, the activation completely vanished. \n",
    "\n",
    "To sum it up, if weights are initialized too large, the network won't learn well. The same happens when weights are initialized too small. How can we find the sweet spot?\n",
    "\n",
    "Remember that as mentioned above, the math required to complete a forward pass through a neural network entails nothing more than a succession of matrix multiplications. If we have an output `y` that is the product of a matrix multiplication between our input vector `x` and weight matrix `a`, each element $i$ in `y` is defined as $$y_{i} = \\sum_{k=0}^{n-1}a_{i,k}x_{k}$$\n",
    "where $i$ is a given row-index of weight matrix `a`, $k$ is both a given column-index in weight matrix `a` and element-index in input vector `x`, and $n$ is the range or total number of elements in `x`. This can also be defined in Python as\n",
    "```\n",
    "y[i] = sum([c*d for c,d in zip(a[i], x)])\n",
    "```\n",
    "We can demonstrate that the matrix product of our inputs `x` and weight matrix `a` that we initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections, or $\\sqrt{512}$ in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00889449315816164, 22.629779825053976)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.627416997969522"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property isn't surprising when we consider it in terms of how matrix multiplication is defined: in order to calculate `y` we sum 512 products of the element-wise multiplication of one element of the inputs `x` by one column of the weights `a`. In our example where both `x` and `a` are initialized using standard normal distributions, each of these 512 products would have a mean of 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.004454135600520567, 0.9906728260800209)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)\n",
    "    y = a*x\n",
    "    mean += y.item()\n",
    "    var += y.pow(2).item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then follows that the *sum* of these 512 products would have a mean of 0, variance of 512, and therefore a standard deviation of $\\sqrt{512}$. \n",
    "\n",
    "And this is why in our example above we saw our layer outputs exploding after 29 consecutive matrix multiplications. In the case of our simple 100-layer architecture, what we'd like is for each layer's outputs to have a standard deviation of about 1. This conceivably would allow us to repeat matrix multiplications across as many network layers as we want, without activations exploding or vanishing.\n",
    "\n",
    "If we first scale the weight matrix `a` by dividing all its randomly chosen values by $\\sqrt{512}$, the element-wise multiplication that fills in one element of the outputs `y` would now have a variance of $\\frac{1}{512}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.091224115950183e-05, 0.0020251519136443882)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    a = torch.randn(1)*math.sqrt(1./512)\n",
    "    y = a*x\n",
    "    mean += y.item()\n",
    "    var += y.pow(2).item()\n",
    "mean/10000, var/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the standard deviation of the matrix `y`, which contains each of the 512 values that are generated by way of matrix multiplication of inputs `x` and weights `a`, would be 1. Let's confirm this experimentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005563282515970058, 1.0004933748755085)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)*math.sqrt(1./512)\n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we let's re-run our quick and dirty 100-layer network by scaling our weights, which we first choose at random from standard normal distribution inside [-1,1], by $\\frac{1}{\\sqrt{n}}$, where n is the number of network input connections at a layer, or 512, in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0358), tensor(1.0825))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = torch.randn(512,512) * math.sqrt(1./512)\n",
    "    x = a @ x\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Our layer outputs didn't explode after 100 of our hypothetical layers. Now while at first glance it may seem like at this point we can call it a day. Real-world neural networks aren't quite as simple as our first example may seem to indicate. For the sake of simplicity, activation functions were omitted. However, we'd never do this in real life, as it's thanks to these non-linear activation functions that come at the end of network layers, that neural nets can do such a great job of approximating the intricate functions that can make impressive predictions using real-world inputs, such as the classification of handwriting samples.\n",
    "\n",
    "Now up until a few years ago, most commonly used activation functions were symmetric about a given value, and had ranges that asymptotically approached values that were plus/minus a certain distance from this midpoint. The hyperbolic tangent and soft sign functions exemplify this class of activations:\n",
    "\n",
    "<img src= 'images/softsign_tanh.png' width='300'/>\n",
    "\n",
    "Image source: [Sefik Ilkin Serengil's blog](https://sefiks.com/2017/11/10/softsign-as-a-neural-networks-activation-function/)\n",
    "\n",
    "Let's add a hyperbolic tangent activation function after each layer our hypothetical 100-layer network, and then see what happens when we use our above weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x): return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0034), tensor(0.0613))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.randn(512,512) * math.sqrt(1./512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of activation outputs of the 100th layer is down to about 0.1, but thankfully activations haven't vanished. We should be feeling pretty good about the initialization scheme we just discovered from scratch!\n",
    "\n",
    "As intuitive as this approach may now seem in retrospect, you may be surprised to hear that this was not, in fact, the conventional approach for initializing weight layers as recently as 2010. When Xavier Glorot and Yoshua Bengio published their landmark paper titled ['Understanding the difficulty of training deep feedforward neural networks'](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), the \"commonly used heuristic\" to which they compared their experiments was that of initializing weights from a uniform distribution in [-1,1] and then scaling by $\\frac{1}{\\sqrt{n}}$.\n",
    "\n",
    "It turns out this \"standard\" approach doesn't actually work that well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.7060e-26), tensor(9.9678e-25))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = torch.Tensor(512,512).uniform_(-1, 1) * math.sqrt(1./512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerunning our 100-layer tanh network with the \"standard\" weight initialization caused activation gradients to become infinitessimally small -- they're just about as good as vanished.\n",
    "\n",
    "This poor performance is what spurred Glorot and Bengio to propose their own weight initialization strategy, which they referred to as \"normalized initialization\" in their paper, and which is now popularly termed \"Xavier initialization.\"\n",
    "\n",
    "Xavier initialization initializes a layer's weights with values chosen from a random uniform distribution bounded between $$\\pm\\frac{\\sqrt{6}}{\\sqrt{n_{i} + n_{i+1}}}$$ where $n_{i}$ is the number of incoming network connections, or \"fan-in,\" to the layer, and $n_{i+1}$ is the number of outgoing network connections from that layer, also known as the \"fan-out.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio believed that Xavier weight initialization would  maintain the variance of activations and back-propagated gradients all the way up or down a network. They theorized that this would allow for the successful training of deeper networks, as they observed that Xavier initialization enabled a 5-layer network to  maintain a near identical variance of weight gradients across layers. \n",
    "\n",
    "<img src= 'images/xavier_init.png' width='500'/>\n",
    "\n",
    "Image source: [Glorot & Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "Using \"standard\" initialization brought about a much bigger gap between weight gradients of the network's lower layers, which were higher, and those of its top-most layers, which were approaching zero.\n",
    "\n",
    "<img src= 'images/standard_init.png' width='500'/>\n",
    "\n",
    "Image source: [Glorot & Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "To drive the point home, they demonstrated that networks thus initialized achieved substationally quicker convergence and higher accuracy on the [CIFAR-10 image classification task](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "We can re-run our 100-layer tanh \"network\" once more, this time using Xavier initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m,h): \n",
    "    return torch.Tensor(m, h).uniform_(-1, 1)*math.sqrt(6./(m+h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0011), tensor(0.0813))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = tanh(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our experimental network, Xavier initialization performs pretty identical to our home-grown method that we derived above, where we sampled values from a random normal distribution and scaled by the square root of number of incoming network connections, $n$.\n",
    "\n",
    "Conceptually, when we are using activation functions that are symmetric about zero and have outputs inside [-1,1], such as softsign and the hyperbolic tangent, it makes sense that we'd want our network layer activation outputs to have a mean of 0 and a standard deviation around 1, on average. And this is precisely what our method and the Xavier method enable.\n",
    "\n",
    "But what if we're using ReLU activation functions? Would it still make sense to want to scale random initial weight values by the same factor?\n",
    "\n",
    "<img src= 'images/relu.png' width='300'/>\n",
    "\n",
    "Image source: [Kanchan Sarkar's blog](https://medium.com/@kanchansarkar/relu-not-a-differentiable-function-why-used-in-gradient-based-optimization-7fef3a4cecec)\n",
    "\n",
    "Let's use a ReLU activation instead of tanh in one of our hypothetical network's layers and observe that the expected standard deviation of outputs would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.027289896678925, 16.01248299986557)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512)\n",
    "    y = relu(a @ x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that when using a ReLU activation, a single layer will, on average have standard deviation that's very close to the square root of the number of input connections *divided by two*, or $\\sqrt{\\frac{512}{2}}$ in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(512/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the values in our weight matrix `a` by this number will give our ReLU layer a standard deviation of 1 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5636869582474232, 0.999786368072371)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var = 0.,0.\n",
    "for i in range(10000):\n",
    "    x = torch.randn(512)\n",
    "    a = torch.randn(512,512) * math.sqrt(2/512)\n",
    "    y = relu(a @ x)\n",
    "    mean += y.mean().item()\n",
    "    var += y.pow(2).mean().item()\n",
    "mean/10000, math.sqrt(var/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we showed before, keeping the standard deviation of layers' activations around 1 will allow us to stack several more layers in a deep neural network without gradients exploding or vanishing.\n",
    "\n",
    "In fact, this exploration into how to best scale ReLU activations in order to prevent gradients from exploding or vanishing is what motivated Kaiming He et. al. to in 2015 [propose their own initialization scheme](https://arxiv.org/pdf/1502.01852.pdf) that's tailored for deep neural nets that use asymmetric non-linear activations like ReLU.\n",
    "\n",
    "He et. al. demonstrated that deep networks (e.g a 22-layer CNN) would converge much earlier if the following input weight initialization strategy is employed:\n",
    "\n",
    "1. Create a tensor with the dimensions appropriate for a weight matrix at a given layer.\n",
    "2. Populate the tensor with numbers randomly chosen from a standard distribution.\n",
    "3. Multiply each randomly chosen number by $\\sqrt{\\frac{2}{n}}$ where $n$ is the number of incoming connections coming into a given layer from the previous layer's output (also known as the \"fan-in\").\n",
    "4. Bias tensors are initialized to zero.\n",
    "\n",
    "In regards to training even deeper networks that used ReLUs, He et. al. found that a 30-layer CNN using Xavier initialization stalled completely and didn't learn at all. However, when the same network was initialized as recommended above it did, in fact, begin to learn and converge.\n",
    "\n",
    "We can follow the above directions to implement our own version of Kaiming initialization and verify that it can indeed prevent activation outputs from both exploding or vanishing if we use ReLU instead of tanh at all layers of our hypothetical 100-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming(m,h): \n",
    "    return torch.randn(m,h)*math.sqrt(2./m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2789), tensor(0.4226))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100): \n",
    "    a = kaiming(512, 512)\n",
    "    x = relu(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Kaiming to initialize weights indeed prevents activation outputs from exploding or vanishing, even after 100 linear layers, each with a ReLU activation! \n",
    "\n",
    "Let's see what happens if we were to use Xavier initialization, instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.3571e-16), tensor(7.7803e-16))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "\n",
    "for i in range(100):\n",
    "    a = xavier(512, 512)\n",
    "    x = relu(a @ x)\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! When using Xavier to initialize weights in our hypothetical 100-layer network with ReLUs, activation gradients almost completely vanish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Kaiming Init to our MNIST Task\n",
    "\n",
    "Turning back to the MNIST task, we will first create a simple linear network layer with a ReLU activation function. We'll use Kaiming to  initialize the weights. And as He et. al.'s paper also recommended, we'll initialize the biases to zero.\n",
    "\n",
    "By the way, here's how to figure out our first layer's weight matrix size: Although the MNIST samples were originally 28x28-sized images, before feeding them into our fully-connected network, we first flattened them into single vectors containing 28 x 28 = 784 pixel values. We've already decided that we want our network's hidden layers to be a size of 50. In order to make this happen, we need to multiply our 784-length vector inputs by a weight matrix that has a height (number of rows) that's also 784 and a width (number of columns) that's 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0002), tensor(0.0502), tensor(0.0319), tensor(0.2249))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = kaiming(m, nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = kaiming(nh,1)\n",
    "b2 = torch.zeros(1)\n",
    "w1.mean(), w1.std(), w2.mean(), w2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(w1.mean())\n",
    "test_near_zero(w1.std()-math.sqrt(2./m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear network layer\n",
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a quick test by adding a relu activation to the above layer, plugging in some images into it, and observing the mean and standard deviation of the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5560), tensor(0.8569))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(lin(x_valid, w1, b1))\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick sanity check, just to double check our from-scratch Kaiming init with PyTorch's own Kaiming method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4296), tensor(0.7306))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.zeros(m,nh)\n",
    "# Choose mode='fan_out' (number of layer outputs), \n",
    "# as opposed to 'fan-in' to preserve the variance of \n",
    "# weights in the backwards pass.\n",
    "init.kaiming_normal_(w1, mode='fan_out')\n",
    "s = relu(lin(x_valid, w1, b1))\n",
    "s.mean(), s.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like PyTorch's kaiming method more or less agrees with our homegrown implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a multi-class classification problem, and mean squared error as a loss function won't help us get great performance. Categorical cross-entropy  would be a better choice. However, first let's implement MSE to prove to ourselves that we can get a loss function successfully working with our model, period. In a subsequent notebook we'll upgrade to a more ideal substitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.37 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model(x_valid).shape == torch.Size([x_valid.shape[0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26.1211)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    # Gradient of loss with respect to output of previous layer\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1)/inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # Gradient of ReLU with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # Gradient of matmul (between inputs and weights) with respect \n",
    "    # to the inputs.\n",
    "    inp.g = out.g @ w.t()\n",
    "    # Weights gradient\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    # Bias gradient\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # Forward pass:\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    \n",
    "    # Backward pass:\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set aside copies of of the gradients produced by our custom methods so that we can benchmark them against the gradients calculated by PyTorch's autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to complete our sanity check, let's perform the forward and backward pass once more with PyTorch autograd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    # Forward pass:\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outstanding! It appears that our home-grown methods can calculate the loss gradients just as well as PyTorch autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring Our Model\n",
    "\n",
    "A theme we'll touch back on repeatedly throughout these notebooks is the importance of refactoring our code into concise, easily readable and interpretable components. \n",
    "\n",
    "It's relatively easy to type out several lines of Python that describe a deep learning model or training regime. Many folks have the tendency to then let sit in perpetuity code that should have only been a v.1 rough draft. Taking the time to refactor our deep learning code will make it easier for others to understand both the fine details and general thrust of our code in one sweep. The habit of refactoring also gives us more modular, extensible code that in turn gives us the freedom to more quickly build on and augment our models.\n",
    "\n",
    "Alternatively put, refactored code is:\n",
    "* Easier to maintain\n",
    "* Easier for new contributors/team members to get up to speed on.\n",
    "* Harder to introduce bugs into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Layers as Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp @ self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        \n",
    "        # Note that creating a giant outer-product, just to sum it, \n",
    "        # is computationally inefficient.\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the `backward()` method in this implementation of the linear layer class we calculate the layer's weight gradient by performing an outer product between a tensor containing a layer's input values for each of the 50,000 training images (with shape `torch.Size([50000, 784])`) and a tensor containing the gradients (that flow in from subsequent layers) on that layer's outputs, for each of the 50,000 training images (with shape `torch.Size([50000, 50])`). \n",
    "\n",
    "To perform the outer product, we add an extra final dimension to the first, or inputs, tensor, and also insert a new dimension at the second, or outputs gradient, tensor's second dimension, and then multiply. Summing the result of this multiplication over all 50,000 training images gives us the final tensor containing weight gradients for a given layer after the end of one backward pass.\n",
    "\n",
    "We will show below that using einstein summation is a more efficient approach, and finally, that performing a matrix multiplication between the transpose of the inputs and the output gradients runs about as fast as einstein summation, while being the most concise implementation:\n",
    "\n",
    "```\n",
    "self.w.g = inp.t() @ out.g\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g  = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero out the gradients.\n",
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "# Uses the weights and biases we initialized above with Kaiming\n",
    "model = Model(w1, b1, w2, b2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 1.63 ms, total: 118 ms\n",
      "Wall time: 30.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.12 s, sys: 2.71 s, total: 7.83 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our refactored model still calculates gradients properly.\n",
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Module() class that handles forward() calls for our classes\n",
    "\n",
    "This allows us to take the `__call__` methods out of the above classes, removing some duplicate code. Also, as we will see just below, the Python code of layer classes that contain intuitively named `forward()`/`backward()` methods is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('Not Implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp > 0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): return inp @ self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        \n",
    "        # Einsteim summation is more efficient than the outer \n",
    "        # product we previously had implemented.\n",
    "        self.w.g = torch.einsum('bi,bj->ij', inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "    # Use the loss function to calculate the loss.\n",
    "    def forward(self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    # Find the slope of the loss function for a particular set of \n",
    "    # inputs and their corresponding targets.\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "       \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 2.05 ms, total: 114 ms\n",
      "Wall time: 29.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 490 ms, sys: 62.2 ms, total: 552 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.21985815602837"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.94/0.141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using einstein summation, our the backward pass is nearly 50 times faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting Linear Layer Class without Einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module): \n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): return inp @ self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ms, sys: 1.95 ms, total: 115 ms\n",
      "Wall time: 29.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 508 ms, sys: 71.4 ms, total: 580 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyTorch matmul to compute the weights gradient for our linear layer (`self.w.g = inp.t() @ out.g`) is just as fast as einstein summation but pleasingly more concise. It's always great to not have to program inside of a string (e.g. `'bi,bj->ij'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's own classes\n",
    "\n",
    "Finally, we can run one backward pass using PyTorch's own implementations of the Module class, linear and ReLU layers, and mean squared error loss function, and observe how their performance compares to the classes we wrote from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        self.loss = mse\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x.squeeze(), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 87.1 ms, sys: 1.38 ms, total: 88.5 ms\n",
      "Wall time: 24.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 23 ms, total: 258 ms\n",
      "Wall time: 45.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2017543859649122"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "146/45.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's own implementations can complete a backward pass three times faster than the best refactoring of our homegrown module, layer, and loss classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "We export all the methods we'll need to call in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_fully_connected_my_reimplementation.ipynb to nb_02.py\r\n"
     ]
    }
   ],
   "source": [
    "!./notebook2script_my_reimplementation.py 02_fully_connected_my_reimplementation.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
