{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724f4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import pipeline\n",
    "except:\n",
    "    !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4d337",
   "metadata": {},
   "source": [
    "情感分析是將文本情感分為積極、消極或中性的過程。情緒分析在不同行業有廣泛的應用，例如從產品評論中監控客戶的情緒，甚至在政治中，例如在選舉年衡量公眾對特定主題的興趣。這篇文章的重點是使用 Hugging Face 來完成各種任務，因此我們不會深入討論每個主題，但如果您有興趣深入了解有關情感分析的更多信息，您可以參考這篇文章：\n",
    "https://towardsdatascience.com/sentiment-analysis-intro-and-implementation-ddf648f79327\n",
    "\n",
    "1. Import libraries 導入庫\n",
    "2. Specify the name of the pre-trained model to be used for this specific task指定用於此特定任務的預訓練模型的名稱（即情感分析）\n",
    "3. Specify the task (i.e. sentiment analysis) 指定任務（即情緒分析）\n",
    "4. Specify the sentence, which will be sentiment analyzed 指定將進行情感分析的句子\n",
    "5. Create an instance of pipeline as analyzer 創建 pipeline 的實例作為 analyzer\n",
    "6. Perform the sentiment analysis and save the results as output 執行情感分析並將結果保存為 output\n",
    "7. Return the results 返回結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e53fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8548853993415833}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify pre-trained model to use\n",
    "model = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "# Specify task\n",
    "task = 'sentiment-analysis'\n",
    "\n",
    "# Text to be analyzed\n",
    "input_text = 'Performing NLP tasks using HuggingFace pipeline is super easy!'\n",
    "\n",
    "# Instantiate pipeline\n",
    "analyzer = pipeline(task, model = model)\n",
    "\n",
    "# Store the output of the analysis\n",
    "output = analyzer(input_text)\n",
    "\n",
    "# Return output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9813a9c",
   "metadata": {},
   "source": [
    "reference: https://medium.com/nlplanet/two-minutes-nlp-beginner-intro-to-hugging-face-main-classes-and-functions-fb6a1d5579c4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbf24f",
   "metadata": {},
   "source": [
    "### sentiment-analysis 情緒分析\n",
    "在本文中，我們測試一個包含情感分析任務的 pipeline。為了預測句子的情緒，只需將句子傳遞給模型即可。模型輸出是一個字典列表，其中每個字典都有一個標籤（對於這個特定示例，值為“正”或“負”）和一個分數（即預測標籤的分數）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d704b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter some words hereHello\n",
      "[{'label': 'POSITIVE', 'score': 0.9995185136795044}]\n",
      "POSITIVE with score 0.9995185136795044\n",
      "POSITIVE with score 0.9998742341995239\n",
      "NEGATIVE with score 0.9995265007019043\n"
     ]
    }
   ],
   "source": [
    "# sentiment-analysis\n",
    "pipe = pipeline('sentiment-analysis')\n",
    "text = input('Enter some words here :')\n",
    "\n",
    "\n",
    "# one world \n",
    "out = pipe(text)\n",
    "print(out) \n",
    "print(f\"{out[0]['label']} with score {out[0]['score']}\")\n",
    "    \n",
    "    \n",
    "# sentnece\n",
    "text = pipe([\"I'm so happy today!\", \"I hate U...\"])\n",
    "for out in text:\n",
    "    print(f\"{out['label']} with score {out['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb566f",
   "metadata": {},
   "source": [
    "### Dataset 數據集\n",
    "通過 dataset 庫，我們可以輕鬆下載 NLP 中使用的一些最常見的基準測試。本次測試為 Stanford Sentiment Treebank (SST2)，它由電影評論中的句子和人類對其情感的註釋組成。它使用雙向（正向和負向）類分割，僅具有句子級標籤。我們可以在數據集庫下找到 SST2 數據集，它存儲為 GLUE 數據集的子集。我們使用 load_dataset 函數加載數據集。\n",
    "\n",
    "數據集已經分為訓練集、驗證集和測試集。我們可以使用 split 參數調用 load_dataset 函數來直接獲取我們感興趣的數據集的分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a79406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/cti110016/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02257f770cba4e93879d5e74d776d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/cti110016/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  idx\n",
       "0       hide new secretions from the parental units       0    0\n",
       "1               contains no wit , only labored gags       0    1\n",
       "2  that loves its characters and communicates som...      1    2\n",
       "3  remains utterly satisfied to remain the same t...      0    3\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0    4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "#print(dataset)\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\", split='train')\n",
    "#print(dataset)\n",
    "\n",
    "import pandas as pd # 或是利用 Pandas 探索數據\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dbcd7",
   "metadata": {},
   "source": [
    "### Pipeline on GPU GPU 上的管道\n",
    "\n",
    "現在我們已經加載了有關情感分析的數據集，讓我們嘗試使用情感分析模型。要提取數據集中的句子列表，我們可以訪問其 data 屬性。讓我們預測 500 個句子的情緒並測量需要多長時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82456c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 30s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "%time results = classifier(dataset.data[\"sentence\"].to_pylist()[:500])\n",
    "# CPU times: user 21.9 s, sys: 56.9 ms, total: 22 s\n",
    "# Wall time: 21.8 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d47ead",
   "metadata": {},
   "source": [
    "預測 500 個句子的情緒需要 21.8 秒，平均每秒 23 個句子。不錯，但我們可以利用 GPU 做得更好。為了讓我們的分類器使用 GPU，我們必須使用 pipeline 創建它並傳遞 device=0 ：通過這樣做，我們要求在關聯的 CUDA 設備 ID 上運行模型，其中從零開始的每個id 都映射到CUDA 設備，值-1 與CPU 關聯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11158a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 26s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=0)\n",
    "%time results = classifier(dataset.data[\"sentence\"].to_pylist()[:500])\n",
    "# CPU times: user 4.07 s, sys: 49.6 ms, total: 4.12 s\n",
    "# Wall time: 4.11 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a37a3",
   "metadata": {},
   "source": [
    "### Metrics 指標\n",
    "如果我們想在 SST2 數據集上測試情感分類器的質量該怎麼辦？我們應該使用哪個指標？\n",
    "\n",
    "在 Hugging Face 中，指標和數據集在數據集庫中配對在一起。為了檢索正確的指標，我們可以使用與 load_dataset 函數使用的相同參數來調用 load_metric 函數。\n",
    "\n",
    "然後，我們使用模型做出的預測和直接從數據集中獲取的引用作為參數來調用度量對象的 compute 函數。特別是對於 SST2 數據集，衡量標準是準確性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a97a9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cti110016\\AppData\\Local\\Temp\\ipykernel_20720\\846288641.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"glue\", \"sst2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813296f9b1cc4fd3965dccca24242d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.988}\n"
     ]
    }
   ],
   "source": [
    "metric = datasets.load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "n_samples = 500\n",
    "\n",
    "X = dataset.data[\"sentence\"].to_pylist()[:n_samples]\n",
    "y = dataset.data[\"label\"].to_pylist()[:n_samples]\n",
    "\n",
    "results = classifier(X)\n",
    "predictions = [0 if res[\"label\"] == \"NEGATIVE\" else 1 for res in results]\n",
    "\n",
    "print(metric.compute(predictions=predictions, references=y))\n",
    "# {'accuracy': 0.988}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce8d96",
   "metadata": {},
   "source": [
    "### AutoClasses 自動類 (需要安裝 torch)\n",
    "\n",
    "在底層，管道由 AutoModel 和 AutoTokenizer 類提供支持。 AutoClass （即像 AutoModel 和 AutoTokenizer 這樣的通用類）是一種快捷方式，可以從預訓練模型（或標記生成器）的名稱或路徑中自動檢索其架構。您只需為您的任務選擇適當的 AutoModel 及其與 AutoTokenizer 關聯的標記器：在我們的示例中，由於我們正在對文本進行分類，因此正確的 AutoModel 是 AutoModelForSequenceClassification。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b546d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "except:\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "    #https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a38d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf477415c3143b6b3825c3d4ab1e57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cti110016\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cti110016\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84efb5",
   "metadata": {},
   "source": [
    "我們使用 AutoTokenizer 創建一個標記生成器對象，並使用AutoModelForSequenceClassification 創建一個模型對象。在這兩種情況下，我們所需要做的就是傳遞模型的名稱，庫會管理其他一切。\n",
    "\n",
    "接下來，讓我們看看如何使用分詞器對句子進行分詞。分詞器輸出是一個字典，由 input_ids （即在輸入句子中檢測到的每個標記的id，取自分詞器詞彙表）、 token_type_ids （用於兩個文本的模型中）組成。預測所需的，我們現在可以忽略它們）和 attention_mask （顯示標記化期間發生填充的位置）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b933521",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      2\u001b[0m                      truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoding)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2561\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2647\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2643\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m         )\n\u001b[0;32m   2646\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2648\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2649\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2650\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2651\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2652\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2653\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2654\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2655\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2656\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2657\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2658\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2659\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2660\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2661\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2662\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2663\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2664\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2665\u001b[0m     )\n\u001b[0;32m   2666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2668\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2669\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2685\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2686\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2830\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2831\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2835\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2836\u001b[0m )\n\u001b[1;32m-> 2838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2839\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2840\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2841\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2842\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2843\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2844\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2845\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2848\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2850\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2851\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2852\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2856\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:473\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[1;32m--> 473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_tensors(tensor_type\u001b[38;5;241m=\u001b[39mtensor_type, prepend_batch_axis\u001b[38;5;241m=\u001b[39mprepend_batch_axis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:700\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tensor_type \u001b[38;5;241m==\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     as_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "encoding = tokenizer([\"Hello!\", \"How are you?\"], padding=True,\n",
    "                     truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(encoding)\n",
    "\"\"\"\n",
    "{'input_ids': tensor([[  101, 29155,   106,   102,     0,     0],\n",
    "        [  101, 12548, 10320, 10855,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1]])}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cb091",
   "metadata": {},
   "source": [
    "然後將標記化的句子傳遞給模型，模型輸出預測。這個特定模型輸出五個分數，其中每個分數是人類評論分數從 1 到 5 的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a366f731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mSequenceClassifierOutput(loss=None, logits=tensor([[-0.2410, -0.9115, -0.3269, -0.0462,  1.2899],\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        [-0.3575, -0.6521, -0.4409,  0.0471,  0.9552]],\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoding' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoding)\n",
    "print(outputs)\n",
    "\"\"\"\n",
    "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2410, -0.9115, -0.3269, -0.0462,  1.2899],\n",
    "        [-0.3575, -0.6521, -0.4409,  0.0471,  0.9552]],\n",
    "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4629e0e",
   "metadata": {},
   "source": [
    "### Save and load models locally 本地保存和加載模型\n",
    "最後，我們看看如何在本地保存模型。這可以使用分詞器和模型的 save_pretrained 函數來完成。如果您想加載之前保存的模型，可以使用右側 AutoModel 類的 from_pretrained 函數加載它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./model\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6e9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
