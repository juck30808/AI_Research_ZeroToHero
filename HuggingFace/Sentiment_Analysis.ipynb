{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724f4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import pipeline\n",
    "except:\n",
    "    !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4d337",
   "metadata": {},
   "source": [
    "æƒ…æ„Ÿåˆ†ææ˜¯å°‡æ–‡æœ¬æƒ…æ„Ÿåˆ†ç‚ºç©æ¥µã€æ¶ˆæ¥µæˆ–ä¸­æ€§çš„éç¨‹ã€‚æƒ…ç·’åˆ†æåœ¨ä¸åŒè¡Œæ¥­æœ‰å»£æ³›çš„æ‡‰ç”¨ï¼Œä¾‹å¦‚å¾ç”¢å“è©•è«–ä¸­ç›£æ§å®¢æˆ¶çš„æƒ…ç·’ï¼Œç”šè‡³åœ¨æ”¿æ²»ä¸­ï¼Œä¾‹å¦‚åœ¨é¸èˆ‰å¹´è¡¡é‡å…¬çœ¾å°ç‰¹å®šä¸»é¡Œçš„èˆˆè¶£ã€‚é€™ç¯‡æ–‡ç« çš„é‡é»æ˜¯ä½¿ç”¨ Hugging Face ä¾†å®Œæˆå„ç¨®ä»»å‹™ï¼Œå› æ­¤æˆ‘å€‘ä¸æœƒæ·±å…¥è¨è«–æ¯å€‹ä¸»é¡Œï¼Œä½†å¦‚æœæ‚¨æœ‰èˆˆè¶£æ·±å…¥äº†è§£æœ‰é—œæƒ…æ„Ÿåˆ†æçš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥åƒè€ƒé€™ç¯‡æ–‡ç« ï¼š\n",
    "https://towardsdatascience.com/sentiment-analysis-intro-and-implementation-ddf648f79327\n",
    "\n",
    "1. Import libraries å°å…¥åº«\n",
    "2. Specify the name of the pre-trained model to be used for this specific taskæŒ‡å®šç”¨æ–¼æ­¤ç‰¹å®šä»»å‹™çš„é è¨“ç·´æ¨¡å‹çš„åç¨±ï¼ˆå³æƒ…æ„Ÿåˆ†æï¼‰\n",
    "3. Specify the task (i.e. sentiment analysis) æŒ‡å®šä»»å‹™ï¼ˆå³æƒ…ç·’åˆ†æï¼‰\n",
    "4. Specify the sentence, which will be sentiment analyzed æŒ‡å®šå°‡é€²è¡Œæƒ…æ„Ÿåˆ†æçš„å¥å­\n",
    "5. Create an instance of pipeline as analyzer å‰µå»º pipeline çš„å¯¦ä¾‹ä½œç‚º analyzer\n",
    "6. Perform the sentiment analysis and save the results as output åŸ·è¡Œæƒ…æ„Ÿåˆ†æä¸¦å°‡çµæœä¿å­˜ç‚º output\n",
    "7. Return the results è¿”å›çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e53fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8548853993415833}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify pre-trained model to use\n",
    "model = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "# Specify task\n",
    "task = 'sentiment-analysis'\n",
    "\n",
    "# Text to be analyzed\n",
    "input_text = 'Performing NLP tasks using HuggingFace pipeline is super easy!'\n",
    "\n",
    "# Instantiate pipeline\n",
    "analyzer = pipeline(task, model = model)\n",
    "\n",
    "# Store the output of the analysis\n",
    "output = analyzer(input_text)\n",
    "\n",
    "# Return output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9813a9c",
   "metadata": {},
   "source": [
    "reference: https://medium.com/nlplanet/two-minutes-nlp-beginner-intro-to-hugging-face-main-classes-and-functions-fb6a1d5579c4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbf24f",
   "metadata": {},
   "source": [
    "### sentiment-analysis æƒ…ç·’åˆ†æ\n",
    "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æ¸¬è©¦ä¸€å€‹åŒ…å«æƒ…æ„Ÿåˆ†æä»»å‹™çš„ pipelineã€‚ç‚ºäº†é æ¸¬å¥å­çš„æƒ…ç·’ï¼Œåªéœ€å°‡å¥å­å‚³éçµ¦æ¨¡å‹å³å¯ã€‚æ¨¡å‹è¼¸å‡ºæ˜¯ä¸€å€‹å­—å…¸åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å­—å…¸éƒ½æœ‰ä¸€å€‹æ¨™ç±¤ï¼ˆå°æ–¼é€™å€‹ç‰¹å®šç¤ºä¾‹ï¼Œå€¼ç‚ºâ€œæ­£â€æˆ–â€œè² â€ï¼‰å’Œä¸€å€‹åˆ†æ•¸ï¼ˆå³é æ¸¬æ¨™ç±¤çš„åˆ†æ•¸ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d704b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter some words hereHello\n",
      "[{'label': 'POSITIVE', 'score': 0.9995185136795044}]\n",
      "POSITIVE with score 0.9995185136795044\n",
      "POSITIVE with score 0.9998742341995239\n",
      "NEGATIVE with score 0.9995265007019043\n"
     ]
    }
   ],
   "source": [
    "# sentiment-analysis\n",
    "pipe = pipeline('sentiment-analysis')\n",
    "text = input('Enter some words here :')\n",
    "\n",
    "\n",
    "# one world \n",
    "out = pipe(text)\n",
    "print(out) \n",
    "print(f\"{out[0]['label']} with score {out[0]['score']}\")\n",
    "    \n",
    "    \n",
    "# sentnece\n",
    "text = pipe([\"I'm so happy today!\", \"I hate U...\"])\n",
    "for out in text:\n",
    "    print(f\"{out['label']} with score {out['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb566f",
   "metadata": {},
   "source": [
    "### Dataset æ•¸æ“šé›†\n",
    "é€šé dataset åº«ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†ä¸‹è¼‰ NLP ä¸­ä½¿ç”¨çš„ä¸€äº›æœ€å¸¸è¦‹çš„åŸºæº–æ¸¬è©¦ã€‚æœ¬æ¬¡æ¸¬è©¦ç‚º Stanford Sentiment Treebank (SST2)ï¼Œå®ƒç”±é›»å½±è©•è«–ä¸­çš„å¥å­å’Œäººé¡å°å…¶æƒ…æ„Ÿçš„è¨»é‡‹çµ„æˆã€‚å®ƒä½¿ç”¨é›™å‘ï¼ˆæ­£å‘å’Œè² å‘ï¼‰é¡åˆ†å‰²ï¼Œåƒ…å…·æœ‰å¥å­ç´šæ¨™ç±¤ã€‚æˆ‘å€‘å¯ä»¥åœ¨æ•¸æ“šé›†åº«ä¸‹æ‰¾åˆ° SST2 æ•¸æ“šé›†ï¼Œå®ƒå­˜å„²ç‚º GLUE æ•¸æ“šé›†çš„å­é›†ã€‚æˆ‘å€‘ä½¿ç”¨ load_dataset å‡½æ•¸åŠ è¼‰æ•¸æ“šé›†ã€‚\n",
    "\n",
    "æ•¸æ“šé›†å·²ç¶“åˆ†ç‚ºè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ã€‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨ split åƒæ•¸èª¿ç”¨ load_dataset å‡½æ•¸ä¾†ç›´æ¥ç²å–æˆ‘å€‘æ„Ÿèˆˆè¶£çš„æ•¸æ“šé›†çš„åˆ†å‰²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a79406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/cti110016/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02257f770cba4e93879d5e74d776d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/cti110016/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichÃ©s the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  idx\n",
       "0       hide new secretions from the parental units       0    0\n",
       "1               contains no wit , only labored gags       0    1\n",
       "2  that loves its characters and communicates som...      1    2\n",
       "3  remains utterly satisfied to remain the same t...      0    3\n",
       "4  on the worst revenge-of-the-nerds clichÃ©s the ...      0    4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "#print(dataset)\n",
    "\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\", split='train')\n",
    "#print(dataset)\n",
    "\n",
    "import pandas as pd # æˆ–æ˜¯åˆ©ç”¨ Pandas æ¢ç´¢æ•¸æ“š\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dbcd7",
   "metadata": {},
   "source": [
    "### Pipeline on GPU GPU ä¸Šçš„ç®¡é“\n",
    "\n",
    "ç¾åœ¨æˆ‘å€‘å·²ç¶“åŠ è¼‰äº†æœ‰é—œæƒ…æ„Ÿåˆ†æçš„æ•¸æ“šé›†ï¼Œè®“æˆ‘å€‘å˜—è©¦ä½¿ç”¨æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚è¦æå–æ•¸æ“šé›†ä¸­çš„å¥å­åˆ—è¡¨ï¼Œæˆ‘å€‘å¯ä»¥è¨ªå•å…¶ data å±¬æ€§ã€‚è®“æˆ‘å€‘é æ¸¬ 500 å€‹å¥å­çš„æƒ…ç·’ä¸¦æ¸¬é‡éœ€è¦å¤šé•·æ™‚é–“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82456c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 30s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "%time results = classifier(dataset.data[\"sentence\"].to_pylist()[:500])\n",
    "# CPU times: user 21.9 s, sys: 56.9 ms, total: 22 s\n",
    "# Wall time: 21.8 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d47ead",
   "metadata": {},
   "source": [
    "é æ¸¬ 500 å€‹å¥å­çš„æƒ…ç·’éœ€è¦ 21.8 ç§’ï¼Œå¹³å‡æ¯ç§’ 23 å€‹å¥å­ã€‚ä¸éŒ¯ï¼Œä½†æˆ‘å€‘å¯ä»¥åˆ©ç”¨ GPU åšå¾—æ›´å¥½ã€‚ç‚ºäº†è®“æˆ‘å€‘çš„åˆ†é¡å™¨ä½¿ç”¨ GPUï¼Œæˆ‘å€‘å¿…é ˆä½¿ç”¨ pipeline å‰µå»ºå®ƒä¸¦å‚³é device=0 ï¼šé€šéé€™æ¨£åšï¼Œæˆ‘å€‘è¦æ±‚åœ¨é—œè¯çš„ CUDA è¨­å‚™ ID ä¸Šé‹è¡Œæ¨¡å‹ï¼Œå…¶ä¸­å¾é›¶é–‹å§‹çš„æ¯å€‹id éƒ½æ˜ å°„åˆ°CUDA è¨­å‚™ï¼Œå€¼-1 èˆ‡CPU é—œè¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11158a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 26s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", device=0)\n",
    "%time results = classifier(dataset.data[\"sentence\"].to_pylist()[:500])\n",
    "# CPU times: user 4.07 s, sys: 49.6 ms, total: 4.12 s\n",
    "# Wall time: 4.11 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a37a3",
   "metadata": {},
   "source": [
    "### Metrics æŒ‡æ¨™\n",
    "å¦‚æœæˆ‘å€‘æƒ³åœ¨ SST2 æ•¸æ“šé›†ä¸Šæ¸¬è©¦æƒ…æ„Ÿåˆ†é¡å™¨çš„è³ªé‡è©²æ€éº¼è¾¦ï¼Ÿæˆ‘å€‘æ‡‰è©²ä½¿ç”¨å“ªå€‹æŒ‡æ¨™ï¼Ÿ\n",
    "\n",
    "åœ¨ Hugging Face ä¸­ï¼ŒæŒ‡æ¨™å’Œæ•¸æ“šé›†åœ¨æ•¸æ“šé›†åº«ä¸­é…å°åœ¨ä¸€èµ·ã€‚ç‚ºäº†æª¢ç´¢æ­£ç¢ºçš„æŒ‡æ¨™ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨èˆ‡ load_dataset å‡½æ•¸ä½¿ç”¨çš„ç›¸åŒåƒæ•¸ä¾†èª¿ç”¨ load_metric å‡½æ•¸ã€‚\n",
    "\n",
    "ç„¶å¾Œï¼Œæˆ‘å€‘ä½¿ç”¨æ¨¡å‹åšå‡ºçš„é æ¸¬å’Œç›´æ¥å¾æ•¸æ“šé›†ä¸­ç²å–çš„å¼•ç”¨ä½œç‚ºåƒæ•¸ä¾†èª¿ç”¨åº¦é‡å°è±¡çš„ compute å‡½æ•¸ã€‚ç‰¹åˆ¥æ˜¯å°æ–¼ SST2 æ•¸æ“šé›†ï¼Œè¡¡é‡æ¨™æº–æ˜¯æº–ç¢ºæ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a97a9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cti110016\\AppData\\Local\\Temp\\ipykernel_20720\\846288641.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"glue\", \"sst2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813296f9b1cc4fd3965dccca24242d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.988}\n"
     ]
    }
   ],
   "source": [
    "metric = datasets.load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "n_samples = 500\n",
    "\n",
    "X = dataset.data[\"sentence\"].to_pylist()[:n_samples]\n",
    "y = dataset.data[\"label\"].to_pylist()[:n_samples]\n",
    "\n",
    "results = classifier(X)\n",
    "predictions = [0 if res[\"label\"] == \"NEGATIVE\" else 1 for res in results]\n",
    "\n",
    "print(metric.compute(predictions=predictions, references=y))\n",
    "# {'accuracy': 0.988}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce8d96",
   "metadata": {},
   "source": [
    "### AutoClasses è‡ªå‹•é¡ (éœ€è¦å®‰è£ torch)\n",
    "\n",
    "åœ¨åº•å±¤ï¼Œç®¡é“ç”± AutoModel å’Œ AutoTokenizer é¡æä¾›æ”¯æŒã€‚ AutoClass ï¼ˆå³åƒ AutoModel å’Œ AutoTokenizer é€™æ¨£çš„é€šç”¨é¡ï¼‰æ˜¯ä¸€ç¨®å¿«æ·æ–¹å¼ï¼Œå¯ä»¥å¾é è¨“ç·´æ¨¡å‹ï¼ˆæˆ–æ¨™è¨˜ç”Ÿæˆå™¨ï¼‰çš„åç¨±æˆ–è·¯å¾‘ä¸­è‡ªå‹•æª¢ç´¢å…¶æ¶æ§‹ã€‚æ‚¨åªéœ€ç‚ºæ‚¨çš„ä»»å‹™é¸æ“‡é©ç•¶çš„ AutoModel åŠå…¶èˆ‡ AutoTokenizer é—œè¯çš„æ¨™è¨˜å™¨ï¼šåœ¨æˆ‘å€‘çš„ç¤ºä¾‹ä¸­ï¼Œç”±æ–¼æˆ‘å€‘æ­£åœ¨å°æ–‡æœ¬é€²è¡Œåˆ†é¡ï¼Œå› æ­¤æ­£ç¢ºçš„ AutoModel æ˜¯ AutoModelForSequenceClassificationã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b546d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "except:\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "    #https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a38d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf477415c3143b6b3825c3d4ab1e57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cti110016\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cti110016\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84efb5",
   "metadata": {},
   "source": [
    "æˆ‘å€‘ä½¿ç”¨ AutoTokenizer å‰µå»ºä¸€å€‹æ¨™è¨˜ç”Ÿæˆå™¨å°è±¡ï¼Œä¸¦ä½¿ç”¨AutoModelForSequenceClassification å‰µå»ºä¸€å€‹æ¨¡å‹å°è±¡ã€‚åœ¨é€™å…©ç¨®æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘æ‰€éœ€è¦åšçš„å°±æ˜¯å‚³éæ¨¡å‹çš„åç¨±ï¼Œåº«æœƒç®¡ç†å…¶ä»–ä¸€åˆ‡ã€‚\n",
    "\n",
    "æ¥ä¸‹ä¾†ï¼Œè®“æˆ‘å€‘çœ‹çœ‹å¦‚ä½•ä½¿ç”¨åˆ†è©å™¨å°å¥å­é€²è¡Œåˆ†è©ã€‚åˆ†è©å™¨è¼¸å‡ºæ˜¯ä¸€å€‹å­—å…¸ï¼Œç”± input_ids ï¼ˆå³åœ¨è¼¸å…¥å¥å­ä¸­æª¢æ¸¬åˆ°çš„æ¯å€‹æ¨™è¨˜çš„idï¼Œå–è‡ªåˆ†è©å™¨è©å½™è¡¨ï¼‰ã€ token_type_ids ï¼ˆç”¨æ–¼å…©å€‹æ–‡æœ¬çš„æ¨¡å‹ä¸­ï¼‰çµ„æˆã€‚é æ¸¬æ‰€éœ€çš„ï¼Œæˆ‘å€‘ç¾åœ¨å¯ä»¥å¿½ç•¥å®ƒå€‘ï¼‰å’Œ attention_mask ï¼ˆé¡¯ç¤ºæ¨™è¨˜åŒ–æœŸé–“ç™¼ç”Ÿå¡«å……çš„ä½ç½®ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b933521",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      2\u001b[0m                      truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoding)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2561\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2647\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2643\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m         )\n\u001b[0;32m   2646\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2648\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2649\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2650\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2651\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2652\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2653\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2654\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2655\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2656\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2657\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2658\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2659\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2660\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2661\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2662\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2663\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2664\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2665\u001b[0m     )\n\u001b[0;32m   2666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2668\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2669\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2685\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2686\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2830\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2831\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2835\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2836\u001b[0m )\n\u001b[1;32m-> 2838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2839\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2840\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2841\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2842\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2843\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2844\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2845\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2846\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2847\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2848\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2849\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2850\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2851\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2852\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2853\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2854\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2856\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:473\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[1;32m--> 473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_tensors(tensor_type\u001b[38;5;241m=\u001b[39mtensor_type, prepend_batch_axis\u001b[38;5;241m=\u001b[39mprepend_batch_axis)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:700\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tensor_type \u001b[38;5;241m==\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     as_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "encoding = tokenizer([\"Hello!\", \"How are you?\"], padding=True,\n",
    "                     truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(encoding)\n",
    "\"\"\"\n",
    "{'input_ids': tensor([[  101, 29155,   106,   102,     0,     0],\n",
    "        [  101, 12548, 10320, 10855,   136,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1]])}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cb091",
   "metadata": {},
   "source": [
    "ç„¶å¾Œå°‡æ¨™è¨˜åŒ–çš„å¥å­å‚³éçµ¦æ¨¡å‹ï¼Œæ¨¡å‹è¼¸å‡ºé æ¸¬ã€‚é€™å€‹ç‰¹å®šæ¨¡å‹è¼¸å‡ºäº”å€‹åˆ†æ•¸ï¼Œå…¶ä¸­æ¯å€‹åˆ†æ•¸æ˜¯äººé¡è©•è«–åˆ†æ•¸å¾ 1 åˆ° 5 çš„æ¦‚ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a366f731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mSequenceClassifierOutput(loss=None, logits=tensor([[-0.2410, -0.9115, -0.3269, -0.0462,  1.2899],\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        [-0.3575, -0.6521, -0.4409,  0.0471,  0.9552]],\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoding' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoding)\n",
    "print(outputs)\n",
    "\"\"\"\n",
    "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2410, -0.9115, -0.3269, -0.0462,  1.2899],\n",
    "        [-0.3575, -0.6521, -0.4409,  0.0471,  0.9552]],\n",
    "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4629e0e",
   "metadata": {},
   "source": [
    "### Save and load models locally æœ¬åœ°ä¿å­˜å’ŒåŠ è¼‰æ¨¡å‹\n",
    "æœ€å¾Œï¼Œæˆ‘å€‘çœ‹çœ‹å¦‚ä½•åœ¨æœ¬åœ°ä¿å­˜æ¨¡å‹ã€‚é€™å¯ä»¥ä½¿ç”¨åˆ†è©å™¨å’Œæ¨¡å‹çš„ save_pretrained å‡½æ•¸ä¾†å®Œæˆã€‚å¦‚æœæ‚¨æƒ³åŠ è¼‰ä¹‹å‰ä¿å­˜çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å³å´ AutoModel é¡çš„ from_pretrained å‡½æ•¸åŠ è¼‰å®ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./model\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6e9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
