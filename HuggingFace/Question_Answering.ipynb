{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ed4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import pipeline\n",
    "except:\n",
    "    !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6765a7",
   "metadata": {},
   "source": [
    "顧名思義，問答是模型回答用戶提供的問題的任務。問答任務一般有兩種類型：\n",
    "\n",
    "1. Extractive (i.e. context-dependent): Where the user describes a situation to the model in the question/prompt and ask the model to generate a response, given that provided information. In this scenario, the model picks the relevant parts of the information from the prompt and returns the results 提取式（即上下文相關）：用戶在問題/提示中向模型描述情況，並要求模型根據所提供的信息生成響應。在這種情況下，模型從提示中選取信息的相關部分並返回結果\n",
    "\n",
    "2. Abstractive (i.e. context-independent): Where the user asks a question from the model, without providing any context 抽象（即上下文無關）：用戶在不提供任何上下文的情況下向模型提出問題\n",
    "\n",
    "Implementation process is similar to the language modeling task. We will use two different models to be able to compare the results. \n",
    "實現過程與語言建模任務類似。我們將使用兩種不同的模型來比較結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be67f1",
   "metadata": {},
   "source": [
    "### 模型1 Question Answering — Implementation\n",
    "讓我們從 distilbert-base-cased-distilled-squad 開始。\n",
    "https://huggingface.co/distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59dcc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b955b32db5464a61ae404d8bb8f80d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cti110016\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cti110016\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3913c488c5f143d98feb57314e4ca141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf00dfdb717347acab4f98a3340da8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7f8dfe26954ae094a971a4274c6325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8dd9ff2f148888487639d3e9d34a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.3341, start: 65, end: 92, answer: natural language processing\n"
     ]
    }
   ],
   "source": [
    "# Specify model\n",
    "model = 'distilbert-base-cased-distilled-squad'\n",
    "\n",
    "# Instantiate pipeline\n",
    "answerer = pipeline(model = model, task=\"question-answering\")\n",
    "\n",
    "# Specify question and context\n",
    "question = \"What does NLP stand for?\"\n",
    "context = \"Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages\"\n",
    "\n",
    "# Generate predictions\n",
    "preds = answerer(\n",
    "    question = question,\n",
    "    context = context,\n",
    ")\n",
    "\n",
    "# Return results\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0ebcf",
   "metadata": {},
   "source": [
    "### 模型2\n",
    "let’s implement the same problem using a different model, named deepset/roberta-base-squad2 (link).\n",
    "讓我們使用不同的模型來實現相同的問題，名為 deepset/roberta-base-squad2。\n",
    "https://huggingface.co/deepset/roberta-base-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5b9cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc03b7ed9d9f44fca2366ad4eb6769c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14630759eb09428eb9460101dc601b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692e0da19e404d56bc84ec6a1b2c282b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adde3c282a7040dfb133cf2a49eea1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a2e38fcd2c4e74bffbc909146f0c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a2817575f4caba904fa8087b520a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.04662790149450302,\n",
       " 'start': 65,\n",
       " 'end': 92,\n",
       " 'answer': 'natural language processing'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify model\n",
    "model = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Specify task\n",
    "task = \"question-answering\"\n",
    "\n",
    "# Instantiate pipeline\n",
    "answerer = pipeline(task = task, model = model, tokenizer = model)\n",
    "\n",
    "# Specify input\n",
    "qa_input = {\n",
    "    'question': 'What does NLP stand for?',\n",
    "    'context': 'Today we are talking about machine learning and specifically the natural language processing, which enables computers to understand, process and generate languages'\n",
    "}\n",
    "\n",
    "# Generate predictions\n",
    "output = answerer(qa_input)\n",
    "\n",
    "# Return results\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ac107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
